<em> By Victor Ruelas </em>

The FTK system is a highly parallelized processor. To be able to process the tremendous data rate in the ATLAS detector, about 1 billion proton-proton collisions per second, the trigger system filters out most uninteresting physics. From proton bunches crossing at 40 MHz, the level-1 trigger system reduces this rate to 100 kHz. FTK reconstructs tracks from data in the inner Pixel, IBL, and SCT detectors after every level-1 accepted event, and feeds the tracking information to the High Level Trigger (HLT). The ATLAS inner detector geometry is mapped to 64 spatial regions using a hardware layer of the FTK system called the Data Formatter (DF). After the mapping step, each region is processed independently. In the Data Formatter, 128 Input Mezzanine (IM) cards or lanes perform clustering. Clustering is the process of figuring out where a particle interacted with the detector. Data sharing for the overlap of neighboring regions to avoid inefficiencies due to various track curvatures and the finite size of the luminous regions of the beam in the z coordinate is also performed by the DF boards. A full mesh Advanced Telecom Computing Architecture (ATCA) backplane interconnect is implemented for the communication of DF boards within a create. There are 8 DF boards in one create. Four crates communicate using fiber links. The data sharing of all the DF boards creates a network.

Each time a DF board is inserted in a create, it needs to be configured with information such as its physical location in the crate and the DF board network, what SCT and Pixel modules are connected to it, and what information should be processed by that particular DF board or get sent to a different board for processing. There are different processes which produce the final configuration of a DF board, the whole process is depicted in the diagram shown below.

<figure>
<img src="pictures/posts/Project_DFDB_Design.png">
<figcaption> Plan for the Data Formatter (DF) Database Project. Now: Information is extracted for the Input Mezzanine (IM) cards and Data Formatter from SCT and Pixel detectors. Information for the IM consists of a map of Read-Out Buffers (ROBs) to Pixel and SCT modules. DF information is a map of SCT and Pixel modules to FTK regions. This is then converted to a text file made up of module lists which is used by two pieces of software to create DF configuration txt files and Read-Out Driver (ROD) Look Up Tables (LUT). These output files are the input for the FtkDFApi software which controls the DF. New: Maps of ROB to Pixel and SCT modules and map of modules to FTK regions is migrated to a centralized Data Formatter Database. Separate programs are rewritten to a new integrated software which accesses the database. This approach allows for better integration and a more natural design in the DF software.
</figcaption>

</figure>


The project I started, has the aim to create a Data Formatter configurations Database (DFDB). The DFDB will act as a centralized bank of configurations rather than having a myriad of individual configuration txt files. The first step in the project is to migrate the mapping information from SCT and Pixel cabling services, Root LUT in the diagram above, to the DFDB. Over the summer, while at CERN I researched the best implementation of the database and created the first prototype. Back at California State University, Fresno, I created the first working prototype of the database using the COOL API (CERN's own relational database infrastructure) and C++.

Since no one in the group had experience in building such a database, I had to find people at CERN with experience and expertise in database frameworks. I coordinated with people in the FTK group to figure out the specific content and use cases for the database. I carried out my research by writing emails, face-to-face and remote meetings. Once I gathered the specifications and had knowledge of database frameworks, I started designing the first prototype. As I designed and created the first working prototype, I learned about FTK and the ATLAS Experiment more in depth. Through my interactions with scientists from different backgrounds, I started to build a network and realized the great deal of skills involved in undertaking such a massive scientific experiment, the ATLAS Experiment. Just in the FTK group alone, organization and distribution of work is very well managed. There are subgroups which work on a specific part of the system and have their own scheduled deliverables. Then the whole group meets constantly to discuss integration. It is all very beautiful in the way it's orchestrated with the same goal in mind, to build a device that could potentially lead to the next big discovery in physics. I too had a role, and it was such an amazing experience to be part of this great scientific endeavor.